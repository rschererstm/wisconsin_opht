{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "This notebook aims to demonstrate my bridge knowledge between Ophthalmology and Medical Text Processing.\n",
    "\n",
    "The software sample prepares a medical text (such as a retinography report) for semantic analysis or clustering tools.\n",
    "\n",
    "The code is a mere prototype without clinical validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# by: Rafael_Scherer, MD, PhD\n",
    "# date: 20/12/22\n",
    "# version ='1.1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libs import time: 1.13066\n"
     ]
    }
   ],
   "source": [
    "# Libs\n",
    "from time import time\n",
    "start_time = time()\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from functools import wraps\n",
    "from types import FunctionType, MethodType\n",
    "from re import search\n",
    "from spacy import displacy  # Visualizador de relação entre palavras\n",
    "from spacy import load\n",
    "from nltk import data\n",
    "from nltk import RSLPStemmer\n",
    "from csv import reader\n",
    "\n",
    "\n",
    "data.path.append(\"/nltk_data\")\n",
    "\n",
    "nlp = load('en_core_web_md')\n",
    "end_time = time()\n",
    "run_time = round(end_time - start_time, 5)\n",
    "print(f'Libs import time: {run_time}')\n",
    "\n",
    "#Global variables\n",
    "DEBUG = True\n",
    "TRIGGER = 0.1\n",
    "PUNCTUATION = r'!#$&()*@[\\]^_`{|}~'\n",
    "\n",
    "\"\"\"\n",
    "# StopWords\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Maintains terms possibly relevant to image reports\n",
    "keep = ['but', 'or', 'against', 'between', 'into','before', 'after', 'above', 'below','up', 'down', 'in', 'out','over', 'under','no', 'nor', 'not','don', \"don't\",'aren', \"aren't\",'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\",'wasn', \"wasn't\", 'weren', \"weren't\"]\n",
    "\n",
    "STOPWORDS = [w for w in STOPWORDS if w not in keep]\n",
    "\"\"\"\n",
    "\n",
    "STOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'if', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'through', 'during', 'to', 'from', 'on', 'off', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'couldn', \"couldn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "\n",
    "with open('sources/ICD.csv', 'r', encoding='utf-8') as f:  # ICD-10 Database\n",
    "    CIDS = list(reader(f, delimiter=\";\"))\n",
    "\n",
    "\n",
    "# Decorator for timing runtime\n",
    "def timefunc(func):\n",
    "    \"\"\"\n",
    "    timefunc is a decorator for printing function execution time\n",
    "    with accumulated execution time greater than TRIGGER\n",
    "\n",
    "    :param func: decorated function\n",
    "    :return: func\n",
    "     \"\"\"\n",
    "\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time()\n",
    "        ret = func(*args, **kwargs)\n",
    "        end_time = time()\n",
    "        run_time = round(end_time - start_time, 5)\n",
    "        wrapper.calls += 1  # n of executions\n",
    "\n",
    "        # print _time of exec._ or _n. exec. * time of exec._ from func with time superior -> \"trigger\"\n",
    "        if run_time > TRIGGER or run_time * wrapper.calls > TRIGGER:\n",
    "            print(f\"Execution of: {func.__qualname__} -- Time: {run_time} seconds -- N of exec.: {wrapper.calls}\")\n",
    "        return ret\n",
    "\n",
    "    wrapper.calls = 0\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def double_spaces(txt):\n",
    "    \"\"\"\n",
    "    double_spaces takes a text string and returns its version without double spaces or spaces at the beginning and end\n",
    "    :param txt: input text\n",
    "    :return: text without double spaces\n",
    "    \"\"\"\n",
    "    adjusted = \" \".join(txt.split())\n",
    "\n",
    "    return adjusted\n",
    "\n",
    "\n",
    "def double_symbols(txt):\n",
    "    \"\"\"\n",
    "    double_symbols receives a text in string and returns its version without duplication of points, commas and dashes\n",
    "    :param txt: input text\n",
    "    :return: text without double symbols\n",
    "    \"\"\"\n",
    "    adjusted = txt\n",
    "    duplicate = True\n",
    "    while duplicate is True:\n",
    "        adjusted = adjusted.replace(',,', ',')\n",
    "        adjusted = adjusted.replace('..', '.')\n",
    "        adjusted = adjusted.replace('--', '-')\n",
    "        adjusted = adjusted.replace('//', '/')\n",
    "        if ',,' in adjusted or '..' in adjusted or '--' in adjusted or '//' in adjusted:\n",
    "            duplicate = True\n",
    "        else:\n",
    "            duplicate = False\n",
    "    return adjusted\n",
    "\n",
    "\n",
    "def rem_punc(txt):\n",
    "    \"\"\"\n",
    "    rem_punc takes a text string and returns its version without punctuation (eg: \"?/!%$\")\n",
    "    :param txt: input text\n",
    "    :return: string with inconvenient punctuation changed to \".\"\n",
    "    \"\"\"\n",
    "\n",
    "    without_punc = txt.translate(str.maketrans('', '', PUNCTUATION))\n",
    "\n",
    "    return without_punc\n",
    "\n",
    "\n",
    "class MetaClasse(type):\n",
    "    # Print when initializing the metaclass\n",
    "    def __new__(mcs, cls, bases, classdict):\n",
    "        new_cls = super().__new__(mcs, cls, bases, classdict)\n",
    "\n",
    "        # key is attribute name and val is attribute value in attribute dict\n",
    "        # Adds the timefunc decorator to all methods of the class\n",
    "        for key, val in classdict.items():\n",
    "            if DEBUG is True:\n",
    "                if isinstance(val, FunctionType) or isinstance(val, MethodType):\n",
    "                    setattr(new_cls, key, timefunc(val))\n",
    "        return new_cls\n",
    "\n",
    "    # Print for audit when calling a Class\n",
    "    def __call__(cls, *args, **kwds):\n",
    "        if DEBUG is True:\n",
    "            print('Calling Class: ', str(cls))\n",
    "            # print('__call__ *args=', str(args))\n",
    "        return type.__call__(cls, *args, **kwds)\n",
    "\n",
    "\n",
    "class Texto(object, metaclass=MetaClasse):\n",
    "    \"\"\"\n",
    "    Texto Class processes grouped words\n",
    "    \"\"\"\n",
    "    def __init__(self, texto: str):\n",
    "        # Lower case + Remove punctuation marks + Double spaces + Double Symbols\n",
    "        self.texto = double_symbols(double_spaces(rem_punc(texto.lower())))\n",
    "        self.tokenized = word_tokenize(self.texto, language='english')\n",
    "\n",
    "    def datas(self):\n",
    "        \"\"\"\n",
    "        datas takes a text string and returns its version without data that may contain any\n",
    "        data in different date/time formats\n",
    "        :return: text without data in date format, List of Match Object Regex\n",
    "\n",
    "        *Apply after numerical anonymization of the anon_numerico function\n",
    "        \"\"\"\n",
    "\n",
    "        # Wanted numbers\n",
    "        datas = [\n",
    "            '([\\d]{1,2}\\W?(Jan(uary)?|Feb(ruary)?|Mar(ch)?|Apr(il)?|May|Jun(e)?|Jul(y)?|Aug(ust)?|Sep(tember)?|Oct(ober)?|Nov(ember)?|Dec(ember)?)\\W[\\d]{4})',\n",
    "            '([\\d]{1,2}\\W?(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)\\W[\\d]{4})',\n",
    "            '([\\d]{1,2}\\W?(Jan(uary)?|Feb(ruary)?|Mar(ch)?|Apr(il)?|May|Jun(e)?|Jul(y)?|Aug(ust)?|Sep(tember)?|oct(ober)?|Nov(ember)?|Dez(ember)?))',\n",
    "            '([\\d]{1,2}\\W?(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(tember)?|oct(ober)?|nov(ember)?|dez(ember)?))',\n",
    "            '((Jan(uary)?|Feb(ruary)?|Mar(ch)?|Apr(il)?|May|Jun(e)?|Jul(y)?|Aug(ust)?|Sep(tember)?|Oct(ober)?|Nov(ember)?|Dez(ember)?)\\W[\\d]{4})',\n",
    "            '((jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(tember)?|oct(ober)?|nov(ember)?|dez(ember)?)\\W[\\d]{4})',\n",
    "            '[\\d]{1,2}\\W?[\\d]{1,2}\\W?[\\d]{4}', '[\\d]{1,2}\\W?[\\d]{1,2}\\W?[\\d]{2}']\n",
    "\n",
    "\n",
    "        regex = [datas]\n",
    "\n",
    "        new = self.texto\n",
    "        mos = []\n",
    "        for i in range(0, 10):\n",
    "            for q in range(0, len(regex)):\n",
    "                for r in regex[q]:\n",
    "                    mo = search(r, new)\n",
    "                    if mo is not None:\n",
    "                        new = new[:mo.start()] + '****' + new[mo.end():]\n",
    "                        mos.append(mo)\n",
    "\n",
    "        return new, mos\n",
    "\n",
    "\n",
    "    def remove_stopwords(self):\n",
    "        removed = []\n",
    "        for item in self.tokenized:\n",
    "            if item not in STOPWORDS:\n",
    "                removed.append(item)\n",
    "\n",
    "        return removed\n",
    "\n",
    "\n",
    "    def lemalize(self, complete=False):\n",
    "        \"\"\"\n",
    "        lemalize is the function that transforms the words in a text into their canonical form\n",
    "\n",
    "        :param complete: if complete is True, it generates the complete report, if not, it just lemalize\n",
    "        :return: list of words in their canonical form + list of parts of speech\n",
    "        + entities present in the text + relationships[start position in the text, word, related word, start position in the text]\n",
    "        \"\"\"\n",
    "\n",
    "        doc = nlp(self.texto)\n",
    "        texto = self.texto\n",
    "        lemmas = []\n",
    "        gramatic = []\n",
    "        relation = []\n",
    "        for token in doc:\n",
    "            gramatic.append(token.pos_)\n",
    "            if token.pos_ == 'VERB':\n",
    "                lemmas.append(token.lemma_)\n",
    "            else:\n",
    "                lemmas.append(token.orth_)\n",
    "\n",
    "        if complete is True:\n",
    "            displacy.render(doc, style='dep', jupyter=True)\n",
    "            tok_l = doc.to_json()['tokens']\n",
    "\n",
    "            for t in tok_l:\n",
    "                head = tok_l[t['head']]\n",
    "                relation.append(\n",
    "                    [t['start'], texto[t['start']:t['end']], t['dep'], texto[head['start']:head['end']], head['start']])\n",
    "                # print(f\"'{texto[t['start']:t['end']]}' is {t['dep']} of '{texto[head['start']:head['end']]}'\")\n",
    "\n",
    "            return lemmas, gramatic, doc.ents, relation\n",
    "\n",
    "        else:\n",
    "            return lemmas\n",
    "\n",
    "\n",
    "    def stemming(self):\n",
    "        \"\"\"\n",
    "        stemming is a tool for admin use to make words reduced to their stem\n",
    "\n",
    "        *Use tokens to be transformed preferentially after lemalization\n",
    "        :return: final word reduced to its stem\n",
    "        \"\"\"\n",
    "\n",
    "        stemmer = RSLPStemmer()\n",
    "        st = []\n",
    "        for palavra in self.tokenized:\n",
    "            if palavra.isupper() is True:  # Does not stem acronyms\n",
    "                st.append(palavra)\n",
    "            else:\n",
    "                st.append(stemmer.stem(palavra))\n",
    "\n",
    "        return st\n",
    "\n",
    "\n",
    "    def icd(self):\n",
    "        \"\"\"\n",
    "        icd takes a text string and returns its version without data that may contain any\n",
    "        ICD-10 facilitating semantic analysis and extracting tags\n",
    "        :return: text without ICD10, List of Match Object Regex\n",
    "        \"\"\"\n",
    "\n",
    "        # Wanted\n",
    "        cid_format = ['[a-z]\\W?[0-9]{2}\\W?[0-9]?']\n",
    "\n",
    "        regex = [cid_format]\n",
    "\n",
    "        new = self.texto\n",
    "        mos = []\n",
    "        desc = []\n",
    "        for i in range(0, 10):\n",
    "            for q in range(0, len(regex)):\n",
    "                for r in regex[q]:\n",
    "                    mo = search(r, new)\n",
    "                    if mo is not None:\n",
    "                        achado = mo.group()\n",
    "                        adjusted = \"\"\n",
    "                        for pos, char in enumerate(achado):  # Fits the found cid to the search format\n",
    "                            if pos == 0:\n",
    "                                adjusted = adjusted + char\n",
    "                            elif char.isalnum() is True:\n",
    "                                adjusted = adjusted + char\n",
    "                        for cid in CIDS:\n",
    "                            if cid[0].lower() == adjusted:  # Check if it is a valid ICD10\n",
    "                                mos.append(mo)\n",
    "                                desc.append(cid[1])\n",
    "                                new = new[:mo.start()] + '**** ' + new[mo.end():]\n",
    "        return new, mos, desc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Example text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Diagnosis: Age-related macular degeneration exudative in the healing phase (H35.3). Report: Vitreoretinal interface without change in reflectivity;  Sensorineural retina with atrophy of the outer retina!  Attenuated foveal depression; Thickness of the sensorineural retina in the central subfield: 215 µm; RPE-Bruch's membrane complex with irregular fusiform hyperreflective line suggestive of neovascular membrane with fibrosis. Superficial and deep retinal vascularizations with irregularities; Presence of choroidal neovascularization. Last exam on 12/5/2020... \""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = \"Diagnosis: Age-related macular degeneration exudative in the healing phase (H35.3). Report: Vitreoretinal interface without change in reflectivity;  Sensorineural retina with atrophy of the outer retina!  Attenuated foveal depression; Thickness of the sensorineural retina in the central subfield: 215 µm; RPE-Bruch's membrane complex with irregular fusiform hyperreflective line suggestive of neovascular membrane with fibrosis. Superficial and deep retinal vascularizations with irregularities; Presence of choroidal neovascularization. Last exam on 12/5/2020... \"\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Lower case + Remove punctuation marks + Double spaces + Double Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling Class:  <class '__main__.Texto'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"diagnosis: age-related macular degeneration exudative in the healing phase h35.3. report: vitreoretinal interface without change in reflectivity; sensorineural retina with atrophy of the outer retina attenuated foveal depression; thickness of the sensorineural retina in the central subfield: 215 µm; rpe-bruch's membrane complex with irregular fusiform hyperreflective line suggestive of neovascular membrane with fibrosis. superficial and deep retinal vascularizations with irregularities; presence of choroidal neovascularization. last exam on 12/5/2020.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc = Texto(report)\n",
    "proc.texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Remove and identify dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"diagnosis: age-related macular degeneration exudative in the healing phase h35.3. report: vitreoretinal interface without change in reflectivity; sensorineural retina with atrophy of the outer retina attenuated foveal depression; thickness of the sensorineural retina in the central subfield: 215 µm; rpe-bruch's membrane complex with irregular fusiform hyperreflective line suggestive of neovascular membrane with fibrosis. superficial and deep retinal vascularizations with irregularities; presence of choroidal neovascularization. last exam on ****.\",\n",
       " [<re.Match object; span=(547, 556), match='12/5/2020'>])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc.datas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Remove and identify ICD-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"diagnosis: age-related macular degeneration exudative in the healing phase **** . report: vitreoretinal interface without change in reflectivity; sensorineural retina with atrophy of the outer retina attenuated foveal depression; thickness of the sensorineural retina in the central subfield: 215 µm; rpe-bruch's membrane complex with irregular fusiform hyperreflective line suggestive of neovascular membrane with fibrosis. superficial and deep retinal vascularizations with irregularities; presence of choroidal neovascularization. last exam on 12/5/2020.\",\n",
       " [<re.Match object; span=(75, 80), match='h35.3'>],\n",
       " ['Degeneration of macula and posterior pole'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc.icd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Remove less important words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['diagnosis', ':', 'age-related', 'macular', 'degeneration', 'exudative', 'in', 'healing', 'phase', 'h35.3', '.', 'report', ':', 'vitreoretinal', 'interface', 'without', 'change', 'in', 'reflectivity', ';', 'sensorineural', 'retina', 'atrophy', 'outer', 'retina', 'attenuated', 'foveal', 'depression', ';', 'thickness', 'sensorineural', 'retina', 'in', 'central', 'subfield', ':', '215', 'µm', ';', 'rpe-bruch', \"'s\", 'membrane', 'complex', 'irregular', 'fusiform', 'hyperreflective', 'line', 'suggestive', 'neovascular', 'membrane', 'fibrosis', '.', 'superficial', 'deep', 'retinal', 'vascularizations', 'irregularities', ';', 'presence', 'choroidal', 'neovascularization', '.', 'last', 'exam', '12/5/2020', '.']\n"
     ]
    }
   ],
   "source": [
    "print(proc.remove_stopwords())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['diagnosil', ':', 'age-related', 'macul', 'degeneration', 'exudativ', 'in', 'the', 'healing', 'phas', 'h35.3', '.', 'report', ':', 'vitreoret', 'interfac', 'without', 'chang', 'in', 'reflectivity', ';', 'sensorine', 'retin', 'with', 'atrophy', 'of', 'the', 'out', 'retin', 'attenuated', 'fove', 'depression', ';', 'thicknes', 'of', 'the', 'sensorine', 'retin', 'in', 'the', 'centr', 'subfield', ':', '215', 'µm', ';', 'rpe-bruch', \"'s\", 'membran', 'complex', 'with', 'irregul', 'fusiform', 'hyperreflectiv', 'lin', 'suggestiv', 'of', 'neovascul', 'membran', 'with', 'fibrosil', '.', 'superfic', 'and', 'deep', 'ret', 'vascularizatiom', 'with', 'irregulariti', ';', 'presenc', 'of', 'choroid', 'neovascularization', '.', 'last', 'ex', 'on', '12/5/2020', '.']\n"
     ]
    }
   ],
   "source": [
    "print(proc.stemming())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['diagnosis', ':', 'age', '-', 'relate', 'macular', 'degeneration', 'exudative', 'in', 'the', 'healing', 'phase', 'h35.3', '.', 'report', ':', 'vitreoretinal', 'interface', 'without', 'change', 'in', 'reflectivity', ';', 'sensorineural', 'retina', 'with', 'atrophy', 'of', 'the', 'outer', 'retina', 'attenuate', 'foveal', 'depression', ';', 'thickness', 'of', 'the', 'sensorineural', 'retina', 'in', 'the', 'central', 'subfield', ':', '215', 'µm', ';', 'rpe', '-', 'bruch', \"'s\", 'membrane', 'complex', 'with', 'irregular', 'fusiform', 'hyperreflective', 'line', 'suggestive', 'of', 'neovascular', 'membrane', 'with', 'fibrosis', '.', 'superficial', 'and', 'deep', 'retinal', 'vascularizations', 'with', 'irregularities', ';', 'presence', 'of', 'choroidal', 'neovascularization', '.', 'last', 'exam', 'on', '12/5/2020', '.']\n"
     ]
    }
   ],
   "source": [
    "print(proc.lemalize())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}